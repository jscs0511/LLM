{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOXtsV8z6aiH"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.55.2 peft==0.17.0 trl==0.21.0 bitsandbytes==0.47.0 accelerate==1.10.0 vllm==0.10.1 gradio==5.43.0 pydantic==2.11.7\n",
        "!pip install ipython>=8.0 jedi>=0.19"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FWYh1lrMWppi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKm5EZij6t2N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from vllm import LLM, SamplingParams\n",
        "import gradio as gr\n",
        "\n",
        "vllm_model = LLM(\n",
        "    model = \"jsgoodlife0511/llama3.1-tuned-and-merged\",\n",
        "    tokenizer = \"jsgoodlife0511/llama3.1-tuned-and-merged\",\n",
        "    gpu_memory_utilization=0.85, # GPU memory limitation, ex) KV cache\n",
        "    enable_lora = True # Dynimically apply different LoRA adapters\n",
        ")\n",
        "\n",
        "sampling_params = SamplingParams(temperature=0.05, top_p=0.95, max_tokens=256)"
      ],
      "metadata": {
        "id": "d4rNgZM0MQos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm.lora.request import LoRARequest\n",
        "\n",
        "dpo_output_1_base_path = \"/content/drive/MyDrive/dpo_output_1/\"\n",
        "dpo_output_2_base_path = \"/content/drive/MyDrive/dpo_output_2/\"\n",
        "\n",
        "lora_configs = {\n",
        "    \"dpo_output_1_cpk_10\": (1, dpo_output_1_base_path + \"checkpoint-10\"),\n",
        "    \"dpo_output_1_cpk_20\": (2, dpo_output_1_base_path + \"checkpoint-20\"),\n",
        "    \"dpo_output_1_cpk_30\": (3, dpo_output_1_base_path + \"checkpoint-30\"),\n",
        "    \"dpo_output_1_cpk_40\": (4, dpo_output_1_base_path + \"checkpoint-40\"),\n",
        "    \"dpo_output_2_cpk_10\": (5, dpo_output_2_base_path + \"checkpoint-10\"),\n",
        "    \"dpo_output_2_cpk_20\": (6, dpo_output_2_base_path + \"checkpoint-20\"),\n",
        "    \"dpo_output_2_cpk_30\": (7, dpo_output_2_base_path + \"checkpoint-30\"),\n",
        "    \"dpo_output_2_cpk_40\": (8, dpo_output_2_base_path + \"checkpoint-40\")\n",
        "}\n",
        "\n",
        "# Gradio interface will call this function\n",
        "def generate_text(raw_input, temperature = 0.05, top_p = 0.95, max_tokens=256, lora_mode = \"default\"):\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    # Formatting alpaca prompt\n",
        "    alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {}\n",
        "\n",
        "    ### Response:\n",
        "    {}\"\"\"\n",
        "    prompt = alpaca_prompt.format(raw_input,\"\") # Response should be blank\n",
        "    lora_config = {}\n",
        "\n",
        "\n",
        "\n",
        "    if lora_mode != \"default\":\n",
        "        lora_config[\"lora_request\"] = LoRARequest(lora_mode, lora_configs[lora_mode][0], lora_configs[lora_mode][1])\n",
        "\n",
        "    outputs = vllm_model.generate( # attach a QLoRA adapter to the base model in part 2\n",
        "        [prompt],\n",
        "        sampling_params,\n",
        "        **lora_config,\n",
        "    )\n",
        "    return outputs[0].outputs[0].text"
      ],
      "metadata": {
        "id": "TiaplZQtRYfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a sample output\n",
        "generate_test(raw_input = \"What should I consider when building an AI service using an LLM?\", lora_mode = \"dpo_output_1_cpk_30\")"
      ],
      "metadata": {
        "id": "FK8jveAH_c88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a sample output\n",
        "generate_test(raw_input = \"What should I consider when building an AI service using an LLM?\", lora_mode = \"dpo_output_2_cpk_30\")"
      ],
      "metadata": {
        "id": "ixPuZT5b_KQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[ # Parameters for the above 'generate_text' function\n",
        "        gr.Textbox(label=\"Prompt\"),\n",
        "        gr.Slider(0.0, 2.0, value=0.0, label=\"Temperature\"), #\n",
        "        gr.Slider(0.0, 1.0, value=0.95, label=\"Top P\"),\n",
        "        gr.Slider(1, 1000, value=256, step=1, label=\"Max Tokens\"),\n",
        "        gr.Dropdown(\n",
        "            choices=[\"default\"] + list(lora_configs.keys()),\n",
        "            label=\"Model Type\",\n",
        "            value=\"default\"\n",
        "        ),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Fine-tuned PEFT Model Demo\",\n",
        "    description=\"Enter a prompt to generate text using the fine-tuned model.\",\n",
        "    api_name = \"generate\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "s3qL4X4O20TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.close()"
      ],
      "metadata": {
        "id": "hkjpfIefe04M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWn1dRCYjp7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}