{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuuaVA0KbX1Y"
      },
      "outputs": [],
      "source": [
        "!pip install mergekit transformers accelerate bitsandbytes huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-lphYNXMZfu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPsqqSITazdi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8Tu-DlwdRj4"
      },
      "outputs": [],
      "source": [
        "%%writefile config.yaml\n",
        "models:\n",
        "  - model: \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\" #OpenAI/ChatGPT-Mixtral-8x7B\n",
        "    parameters:\n",
        "      weight: 0.5\n",
        "  - model: \"VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct\" #TheFuriousGunner/Mistral-7B-Finetuned\n",
        "    parameters:\n",
        "      weight: 0.25\n",
        "  - model: \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "    parameters:\n",
        "      weight: 0.25\n",
        "\n",
        "merge_method: slerp\n",
        "base_model: \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
        "dtype: bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t-Q3QbjeUKw"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import json\n",
        "\n",
        "config_path = hf_hub_download(\"VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct\", \"config.json\")\n",
        "with open(config_path) as f:\n",
        "    cfg = json.load(f)\n",
        "print(cfg.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcxKq6JimepJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if_YsuOs6tlU"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/mergekit_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P0WKOze2dk7n"
      },
      "outputs": [],
      "source": [
        "#!mergekit-yaml config.yaml ./merged-model\n",
        "!mergekit-yaml config.yaml /content/drive/MyDrive/mergekit_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S3IBd7RVdnR-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"/content/drive/MyDrive/mergekit_model\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "inputs = tokenizer(\"Hello, what is the capital of France?\", return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ViWpxneXmxJo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 원래 모델 중 하나에서 tokenizer 가져오기\n",
        "tokenizer_model = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
        "\n",
        "# merge된 모델 폴더로 tokenizer 저장\n",
        "tokenizer.save_pretrained(\"./merged-model\")\n",
        "\n",
        "print(\"✅ Tokenizer 추가 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lzBjU7hwmIEo"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, login\n",
        "\n",
        "# 1️⃣ HF 로그인\n",
        "login(token='HF_TOKEN')  # 이미 HF_TOKEN 있으니까 그대로 사용\n",
        "\n",
        "# 2️⃣ 업로드 (자동으로 레포지토리 생성)\n",
        "api = HfApi()\n",
        "repo_id = \"jsgoodlife0511/Mergekit-Mixtral-8x7B\"  # 원하는 모델 이름\n",
        "api.upload_folder(\n",
        "    folder_path=\"./merged-model\",  # 로컬에 merge된 모델 폴더\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    ignore_patterns=[\"*.lock\"]\n",
        ")\n",
        "\n",
        "print(f\"✅ 모델 업로드 완료! Hugging Face Hub: https://huggingface.co/{repo_id}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}